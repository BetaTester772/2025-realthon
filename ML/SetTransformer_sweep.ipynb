{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npjXLgLf6EvQ"
   },
   "source": [
    "# SetTransformer Histogram Prediction - W&B Sweep\n",
    "\n",
    "대학 강의에서 학생들이 공유한 점수 샘플을 바탕으로 전체 분포를 예측하는 모델을 W&B Sweep으로 최적화합니다.\n",
    "\n",
    "## 실행 순서\n",
    "1. W&B 로그인\n",
    "2. 필요한 모듈 정의\n",
    "3. Sweep 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "id1_IDTZ6EvR"
   },
   "source": [
    "## 1. 패키지 설치 및 W&B 로그인"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OGiNH0fE6EvS"
   },
   "source": [
    "!pip install wandb torch numpy scipy -q"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KGxJAYkX6EvS",
    "outputId": "81340036-800d-46eb-a56b-efcf0efb47fb"
   },
   "source": [
    "import wandb\n",
    "\n",
    "# W&B 로그인\n",
    "wandb.login()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2grfaPlX6EvS"
   },
   "source": [
    "## 2. 데이터 생성 모듈"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7KlVkRAT6EvT",
    "outputId": "dec95b53-d15f-4e3a-80db-81abe7d12b41"
   },
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "def generate_complex_class_scores(num_students: int = 30) -> Tuple[np.ndarray, str]:\n",
    "    \"\"\"\n",
    "    Generate scores with complex distributions.\n",
    "    \"\"\"\n",
    "    class_type = random.choice([\n",
    "        \"easy\", \"normal\", \"hard\", \"bimodal\",\n",
    "        \"trimodal\", \"uniform\", \"skewed_left\", \"skewed_right\",\n",
    "        \"outliers\", \"clustered\", \"exponential\", \"beta\"\n",
    "    ])\n",
    "\n",
    "    if class_type == \"easy\":\n",
    "        mu = np.random.uniform(75, 90)\n",
    "        sigma = np.random.uniform(5, 10)\n",
    "        scores = np.random.normal(mu, sigma, num_students)\n",
    "\n",
    "    elif class_type == \"normal\":\n",
    "        mu = np.random.uniform(60, 80)\n",
    "        sigma = np.random.uniform(8, 15)\n",
    "        scores = np.random.normal(mu, sigma, num_students)\n",
    "\n",
    "    elif class_type == \"hard\":\n",
    "        mu = np.random.uniform(40, 65)\n",
    "        sigma = np.random.uniform(8, 15)\n",
    "        scores = np.random.normal(mu, sigma, num_students)\n",
    "\n",
    "    elif class_type == \"bimodal\":\n",
    "        mu1 = np.random.uniform(35, 55)\n",
    "        mu2 = np.random.uniform(70, 90)\n",
    "        sigma = np.random.uniform(5, 10)\n",
    "        split = random.randint(num_students // 3, num_students * 2 // 3)\n",
    "        scores1 = np.random.normal(mu1, sigma, split)\n",
    "        scores2 = np.random.normal(mu2, sigma, num_students - split)\n",
    "        scores = np.concatenate([scores1, scores2])\n",
    "\n",
    "    elif class_type == \"trimodal\":\n",
    "        mu1 = np.random.uniform(30, 45)\n",
    "        mu2 = np.random.uniform(55, 70)\n",
    "        mu3 = np.random.uniform(80, 95)\n",
    "        sigma = np.random.uniform(5, 8)\n",
    "        n1 = num_students // 3\n",
    "        n2 = num_students // 3\n",
    "        n3 = num_students - n1 - n2\n",
    "        scores1 = np.random.normal(mu1, sigma, n1)\n",
    "        scores2 = np.random.normal(mu2, sigma, n2)\n",
    "        scores3 = np.random.normal(mu3, sigma, n3)\n",
    "        scores = np.concatenate([scores1, scores2, scores3])\n",
    "\n",
    "    elif class_type == \"uniform\":\n",
    "        low = np.random.uniform(30, 50)\n",
    "        high = np.random.uniform(70, 95)\n",
    "        scores = np.random.uniform(low, high, num_students)\n",
    "\n",
    "    elif class_type == \"skewed_left\":\n",
    "        alpha = 2\n",
    "        beta_param = 5\n",
    "        scores_01 = np.random.beta(beta_param, alpha, num_students)\n",
    "        scores = scores_01 * 60 + 35\n",
    "\n",
    "    elif class_type == \"skewed_right\":\n",
    "        alpha = 5\n",
    "        beta_param = 2\n",
    "        scores_01 = np.random.beta(alpha, beta_param, num_students)\n",
    "        scores = scores_01 * 60 + 30\n",
    "\n",
    "    elif class_type == \"outliers\":\n",
    "        mu = np.random.uniform(60, 75)\n",
    "        sigma = np.random.uniform(8, 12)\n",
    "        scores = np.random.normal(mu, sigma, num_students)\n",
    "        num_outliers = random.randint(2, min(3, num_students // 10))\n",
    "        outlier_indices = random.sample(range(num_students), num_outliers)\n",
    "        for idx in outlier_indices:\n",
    "            if random.random() < 0.5:\n",
    "                scores[idx] = np.random.uniform(0, 30)\n",
    "            else:\n",
    "                scores[idx] = np.random.uniform(95, 100)\n",
    "\n",
    "    elif class_type == \"clustered\":\n",
    "        num_clusters = random.randint(3, 5)\n",
    "        cluster_centers = np.random.uniform(20, 90, num_clusters)\n",
    "        cluster_std = np.random.uniform(3, 6)\n",
    "        students_per_cluster = [num_students // num_clusters] * num_clusters\n",
    "        for i in range(num_students % num_clusters):\n",
    "            students_per_cluster[i] += 1\n",
    "        scores_list = []\n",
    "        for center, n in zip(cluster_centers, students_per_cluster):\n",
    "            cluster_scores = np.random.normal(center, cluster_std, n)\n",
    "            scores_list.append(cluster_scores)\n",
    "        scores = np.concatenate(scores_list)\n",
    "\n",
    "    elif class_type == \"exponential\":\n",
    "        scale = np.random.uniform(15, 25)\n",
    "        scores = np.random.exponential(scale, num_students)\n",
    "        scores = 100 - scores\n",
    "\n",
    "    elif class_type == \"beta\":\n",
    "        alpha = np.random.uniform(2, 5)\n",
    "        beta_param = np.random.uniform(2, 5)\n",
    "        scores_01 = np.random.beta(alpha, beta_param, num_students)\n",
    "        scores = scores_01 * 80 + 10\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown class type: {class_type}\")\n",
    "\n",
    "    scores = np.clip(scores, 0, 100)\n",
    "    return scores, class_type\n",
    "\n",
    "\n",
    "def generate_realistic_class_scores(num_students: int = 30) -> Tuple[np.ndarray, str]:\n",
    "    \"\"\"\n",
    "    Generate scores based on realistic scenarios.\n",
    "    \"\"\"\n",
    "    scenarios = [\n",
    "        \"gifted_program\", \"remedial_class\", \"mixed_ability\",\n",
    "        \"test_anxiety\", \"cheating_suspected\", \"grade_inflation\", \"strict_grading\"\n",
    "    ]\n",
    "    scenario = random.choice(scenarios)\n",
    "\n",
    "    if scenario == \"gifted_program\":\n",
    "        mu = np.random.uniform(85, 95)\n",
    "        sigma = np.random.uniform(3, 7)\n",
    "        scores = np.random.normal(mu, sigma, num_students)\n",
    "\n",
    "    elif scenario == \"remedial_class\":\n",
    "        mu = np.random.uniform(35, 55)\n",
    "        sigma = np.random.uniform(8, 12)\n",
    "        scores = np.random.normal(mu, sigma, num_students)\n",
    "\n",
    "    elif scenario == \"mixed_ability\":\n",
    "        mu = np.random.uniform(55, 70)\n",
    "        sigma = np.random.uniform(15, 25)\n",
    "        scores = np.random.normal(mu, sigma, num_students)\n",
    "\n",
    "    elif scenario == \"test_anxiety\":\n",
    "        prepared = int(num_students * 0.7)\n",
    "        anxious = num_students - prepared\n",
    "        scores_prepared = np.random.normal(75, 10, prepared)\n",
    "        scores_anxious = np.random.normal(50, 15, anxious)\n",
    "        scores = np.concatenate([scores_prepared, scores_anxious])\n",
    "\n",
    "    elif scenario == \"cheating_suspected\":\n",
    "        cheaters = int(num_students * 0.3)\n",
    "        honest = num_students - cheaters\n",
    "        scores_honest = np.random.normal(65, 12, honest)\n",
    "        scores_cheaters = np.random.normal(95, 3, cheaters)\n",
    "        scores = np.concatenate([scores_honest, scores_cheaters])\n",
    "\n",
    "    elif scenario == \"grade_inflation\":\n",
    "        alpha = 2\n",
    "        beta_param = 5\n",
    "        scores_01 = np.random.beta(beta_param, alpha, num_students)\n",
    "        scores = scores_01 * 50 + 50\n",
    "\n",
    "    elif scenario == \"strict_grading\":\n",
    "        alpha = 5\n",
    "        beta_param = 2\n",
    "        scores_01 = np.random.beta(alpha, beta_param, num_students)\n",
    "        scores = scores_01 * 70 + 15\n",
    "\n",
    "    scores = np.clip(scores, 0, 100)\n",
    "    return scores, scenario\n",
    "\n",
    "\n",
    "def scores_to_hist(scores: np.ndarray, num_bins: int = 10) -> np.ndarray:\n",
    "    \"\"\"Convert scores to histogram probability distribution.\"\"\"\n",
    "    bins = np.linspace(0, 100, num_bins + 1)\n",
    "    hist, _ = np.histogram(scores, bins=bins)\n",
    "    hist_prob = hist.astype(np.float32) / len(scores)\n",
    "    return hist_prob\n",
    "\n",
    "\n",
    "def create_sample_target_pair(\n",
    "    num_students: int = 30,\n",
    "    sample_size: int = 10,\n",
    "    num_bins: int = 10\n",
    ") -> Tuple[np.ndarray, np.ndarray, str]:\n",
    "    \"\"\"\n",
    "    Create training sample with advanced distributions.\n",
    "    \"\"\"\n",
    "    distribution_type = random.choice(['complex', 'realistic'])\n",
    "\n",
    "    if distribution_type == 'complex':\n",
    "        scores_all, class_type = generate_complex_class_scores(num_students)\n",
    "    else:\n",
    "        scores_all, class_type = generate_realistic_class_scores(num_students)\n",
    "\n",
    "    sample_size = min(sample_size, num_students)\n",
    "    idx = np.random.choice(len(scores_all), sample_size, replace=False)\n",
    "    sample_scores = scores_all[idx]\n",
    "\n",
    "    sample_scores = np.sort(sample_scores)\n",
    "    sample_scores_norm = sample_scores / 100.0\n",
    "\n",
    "    hist_prob = scores_to_hist(scores_all, num_bins)\n",
    "\n",
    "    return sample_scores_norm.astype(np.float32), hist_prob, class_type\n",
    "\n",
    "print(\"✓ Data generation module loaded\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3j1ctP1l6EvT"
   },
   "source": [
    "## 3. PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PXbD8MX46EvT",
    "outputId": "5e15edb9-8a33-4f9d-ab8d-ad2e3e1d2df3"
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FlexibleScoreDistDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset with variable sample sizes.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = 5000,\n",
    "        num_students_range: Tuple[int, int] = (30, 30),\n",
    "        sample_size_range: Tuple[int, int] = (10, 10),\n",
    "        num_bins: int = 10\n",
    "    ):\n",
    "        self.num_classes = num_classes\n",
    "        self.num_students_range = num_students_range\n",
    "        self.sample_size_range = sample_size_range\n",
    "        self.num_bins = num_bins\n",
    "\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        self.class_types = []\n",
    "        self.metadata = []\n",
    "\n",
    "        print(f\"Generating {num_classes} synthetic classes...\")\n",
    "\n",
    "        for i in range(num_classes):\n",
    "            if (i + 1) % 5000 == 0:\n",
    "                print(f\"  Generated {i + 1}/{num_classes} classes\")\n",
    "\n",
    "            num_students = random.randint(*num_students_range)\n",
    "            sample_size = random.randint(*sample_size_range)\n",
    "            sample_size = min(sample_size, num_students)\n",
    "\n",
    "            sample_scores, hist_prob, class_type = create_sample_target_pair(\n",
    "                num_students=num_students,\n",
    "                sample_size=sample_size,\n",
    "                num_bins=num_bins\n",
    "            )\n",
    "\n",
    "            self.inputs.append(sample_scores)\n",
    "            self.targets.append(hist_prob)\n",
    "            self.class_types.append(class_type)\n",
    "            self.metadata.append({\n",
    "                'sample_size': sample_size,\n",
    "                'num_students': num_students\n",
    "            })\n",
    "\n",
    "        print(f\"Dataset created: {len(self)} samples\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.inputs[idx]\n",
    "        y = self.targets[idx]\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "\n",
    "def collate_fn_flexible(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for variable-length sequences.\n",
    "    \"\"\"\n",
    "    inputs, targets = zip(*batch)\n",
    "\n",
    "    max_len = max(x.size(0) for x in inputs)\n",
    "\n",
    "    padded_inputs = []\n",
    "    masks = []\n",
    "\n",
    "    for x in inputs:\n",
    "        pad_len = max_len - x.size(0)\n",
    "        if pad_len > 0:\n",
    "            padded = torch.cat([x, torch.zeros(pad_len)])\n",
    "            mask = torch.cat([torch.ones(x.size(0)), torch.zeros(pad_len)])\n",
    "        else:\n",
    "            padded = x\n",
    "            mask = torch.ones(x.size(0))\n",
    "\n",
    "        padded_inputs.append(padded)\n",
    "        masks.append(mask)\n",
    "\n",
    "    inputs = torch.stack(padded_inputs)\n",
    "    masks = torch.stack(masks)\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    return inputs, targets, masks\n",
    "\n",
    "print(\"✓ Dataset module loaded\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJd-9gGm6EvU"
   },
   "source": [
    "## 4. SetTransformer 모델"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-gQoscXy6EvU",
    "outputId": "e8e6c4a3-58f7-4cbe-ac82-4ed4c35c2fe3"
   },
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiheadAttentionBlock(nn.Module):\n",
    "    \"\"\"Multi-head attention block.\"\"\"\n",
    "    def __init__(self, dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * 4, dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        if context is None:\n",
    "            context = x\n",
    "        attn_out, _ = self.attention(x, context, context)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        x = self.norm2(x + self.ffn(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class SetTransformerEncoder(nn.Module):\n",
    "    \"\"\"Set Transformer Encoder with Inducing Points.\"\"\"\n",
    "    def __init__(self, dim, num_heads, num_inducers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.inducing_points = nn.Parameter(torch.randn(1, num_inducers, dim))\n",
    "        self.mab1 = MultiheadAttentionBlock(dim, num_heads, dropout)\n",
    "        self.mab2 = MultiheadAttentionBlock(dim, num_heads, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        I = self.inducing_points.expand(batch_size, -1, -1)\n",
    "        H = self.mab1(I, x)\n",
    "        return self.mab2(x, H)\n",
    "\n",
    "\n",
    "class FlexibleHistogramPredictor(nn.Module):\n",
    "    \"\"\"SetTransformer-based histogram predictor.\"\"\"\n",
    "    def __init__(self, hidden_dim=64, num_bins=10, num_heads=4, num_inducers=16, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(1, hidden_dim)\n",
    "        self.encoder = SetTransformerEncoder(hidden_dim, num_heads, num_inducers, dropout)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_bins),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = self.input_proj(x)\n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.decoder(x)\n",
    "\n",
    "print(\"✓ Model module loaded\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_CtDTnz6EvU"
   },
   "source": [
    "## 5. 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-2eHADRp6EvU",
    "outputId": "1e1fc310-5d18-47d3-b9ba-6cce2e9f1fb1"
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        x, y, mask = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x, y, mask = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader.dataset)\n",
    "    return avg_loss\n",
    "\n",
    "print(\"✓ Training functions loaded\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1h1MtddF6EvU"
   },
   "source": [
    "## 6. W&B Sweep 설정"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pTU6Degq6EvU",
    "outputId": "44da61f3-aac9-45a5-f705-920145986b01"
   },
   "source": [
    "SWEEP_CONFIG = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'epochs': {\n",
    "            'min': 10,\n",
    "            'max': 200\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [32, 64, 128]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'min': 1e-5,\n",
    "            'max': 1e-2,\n",
    "            'distribution': 'log_uniform_values'\n",
    "        },\n",
    "        'hidden_dim': {\n",
    "            'values': [32, 64, 128, 256]\n",
    "        },\n",
    "        'num_heads': {\n",
    "            'values': [2, 4, 8]\n",
    "        },\n",
    "        'num_inducers': {\n",
    "            'values': [8, 16, 32, 64]\n",
    "        },\n",
    "        'dropout': {\n",
    "            'min': 0.0,\n",
    "            'max': 0.5\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['adam', 'adamw']\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'min': 0.0,\n",
    "            'max': 1e-3\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✓ Sweep configuration loaded\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WjOs2ZB6EvU"
   },
   "source": [
    "## 7. Sweep 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBY1lAua6EvU",
    "outputId": "5a7343d8-a2a5-4b54-b76a-8b20a3fbf9f6"
   },
   "source": [
    "def train_sweep():\n",
    "    \"\"\"Main training function for W&B sweep.\"\"\"\n",
    "    with wandb.init() as run:\n",
    "        config = wandb.config\n",
    "\n",
    "        set_seed(42)\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        # Create datasets\n",
    "        print(\"\\n=== Creating datasets ===\")\n",
    "        train_dataset = FlexibleScoreDistDataset(\n",
    "            num_classes=20000,\n",
    "            num_students_range=(20, 50),\n",
    "            sample_size_range=(5, 20),\n",
    "            num_bins=10\n",
    "        )\n",
    "        val_dataset = FlexibleScoreDistDataset(\n",
    "            num_classes=5000,\n",
    "            num_students_range=(20, 50),\n",
    "            sample_size_range=(5, 20),\n",
    "            num_bins=10\n",
    "        )\n",
    "\n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn_flexible,\n",
    "            num_workers=2\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn_flexible,\n",
    "            num_workers=2\n",
    "        )\n",
    "\n",
    "        # Create model\n",
    "        model = FlexibleHistogramPredictor(\n",
    "            hidden_dim=config.hidden_dim,\n",
    "            num_bins=10,\n",
    "            num_heads=config.num_heads,\n",
    "            num_inducers=config.num_inducers,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "        model = model.to(device)\n",
    "\n",
    "        num_params = sum(p.numel() for p in model.parameters())\n",
    "        wandb.log({\"model_parameters\": num_params})\n",
    "        print(f\"Model parameters: {num_params:,}\")\n",
    "\n",
    "        # Setup training\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        if config.optimizer == 'adam':\n",
    "            optimizer = torch.optim.Adam(\n",
    "                model.parameters(),\n",
    "                lr=config.learning_rate,\n",
    "                weight_decay=config.weight_decay\n",
    "            )\n",
    "        else:\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                model.parameters(),\n",
    "                lr=config.learning_rate,\n",
    "                weight_decay=config.weight_decay\n",
    "            )\n",
    "\n",
    "        # Training loop\n",
    "        best_val_loss = float('inf')\n",
    "        patience = 10\n",
    "        patience_counter = 0\n",
    "        best_model_state = None\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "            val_loss = validate(model, val_loader, criterion, device)\n",
    "\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            })\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch+1:3d}/{config.epochs} | \"\n",
    "                f\"Train Loss: {train_loss:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f}\"\n",
    "            )\n",
    "\n",
    "            # Early stopping and save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "\n",
    "                # Save best model state\n",
    "                best_model_state = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                    'config': dict(config)\n",
    "                }\n",
    "\n",
    "                # Save to file\n",
    "                torch.save(best_model_state, f'best_model_{run.id}.pt')\n",
    "                print(f\"  → Saved best model (val_loss: {val_loss:.4f})\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "        wandb.log({'best_val_loss': best_val_loss})\n",
    "        print(f\"\\n=== Best validation loss: {best_val_loss:.4f} ===\")\n",
    "\n",
    "        # Save model to W&B artifacts\n",
    "        if best_model_state is not None:\n",
    "            artifact = wandb.Artifact(\n",
    "                name=f'model-{run.id}',\n",
    "                type='model',\n",
    "                metadata=dict(config)\n",
    "            )\n",
    "            artifact.add_file(f'best_model_{run.id}.pt')\n",
    "            run.log_artifact(artifact)\n",
    "            print(f\"✓ Model saved to W&B artifacts\")\n",
    "\n",
    "print(\"✓ Sweep training function loaded\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHra2sCQ6EvV"
   },
   "source": [
    "## 8. Sweep 실행\n",
    "\n",
    "아래 셀을 실행하여 W&B sweep을 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5eu2ggkj6EvV",
    "outputId": "d5982859-3f50-48d8-f9b3-a44c9e8f45ea"
   },
   "source": [
    "# Sweep 설정\n",
    "PROJECT_NAME = \"histogram-prediction-sweep\"\n",
    "SWEEP_COUNT = 40  # 실행할 sweep 개수\n",
    "\n",
    "# Sweep 초기화\n",
    "sweep_id = wandb.sweep(\n",
    "    sweep=SWEEP_CONFIG,\n",
    "    project=PROJECT_NAME\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Created sweep: {sweep_id}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Sweep 실행\n",
    "wandb.agent(\n",
    "    sweep_id,\n",
    "    function=train_sweep,\n",
    "    count=SWEEP_COUNT\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Sweep completed!\")\n",
    "print(f\"{'='*60}\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9. Best 모델 불러오기 및 사용\n",
    "\n",
    "Sweep이 완료된 후 best 모델을 불러와서 사용할 수 있습니다."
   ],
   "metadata": {
    "id": "LpT86moa6EvV"
   }
  },
  {
   "cell_type": "code",
   "source": "def find_best_sweep_run(project_name: str, sweep_id: str):\n    \"\"\"\n    W&B sweep에서 best run을 찾습니다.\n\n    Args:\n        project_name: W&B 프로젝트 이름\n        sweep_id: Sweep ID\n\n    Returns:\n        best_run: Best run 객체\n        best_model_path: Best 모델 파일 경로\n    \"\"\"\n    api = wandb.Api()\n\n    # Sweep 정보 가져오기\n    sweep = api.sweep(f\"{project_name}/{sweep_id}\")\n\n    # 모든 runs 가져오기\n    runs = sweep.runs\n\n    # val_loss 기준으로 정렬\n    best_run = min(runs, key=lambda run: run.summary.get('best_val_loss', float('inf')))\n\n    print(f\"{'='*60}\")\n    print(f\"Best Run Found!\")\n    print(f\"{'='*60}\")\n    print(f\"Run ID: {best_run.id}\")\n    print(f\"Run Name: {best_run.name}\")\n    print(f\"Best Val Loss: {best_run.summary.get('best_val_loss', 'N/A'):.4f}\")\n    print(f\"\\nHyperparameters:\")\n    for key, value in best_run.config.items():\n        print(f\"  {key}: {value}\")\n    print(f\"{'='*60}\")\n\n    # 모델 파일 경로\n    best_model_path = f\"best_model_{best_run.id}.pt\"\n\n    return best_run, best_model_path\n\n\ndef download_best_model_from_wandb(project_name: str, sweep_id: str, download_dir: str = '.'):\n    \"\"\"\n    W&B artifacts에서 best 모델을 다운로드합니다.\n\n    Args:\n        project_name: W&B 프로젝트 이름\n        sweep_id: Sweep ID\n        download_dir: 다운로드할 디렉토리\n\n    Returns:\n        model_path: 다운로드된 모델 파일 경로\n    \"\"\"\n    api = wandb.Api()\n\n    # Best run 찾기\n    best_run, expected_model_path = find_best_sweep_run(project_name, sweep_id)\n\n    # Artifacts에서 모델 다운로드\n    try:\n        artifacts = best_run.logged_artifacts()\n        model_artifact = None\n\n        for artifact in artifacts:\n            if artifact.type == 'model':\n                model_artifact = artifact\n                break\n\n        if model_artifact:\n            # Artifact 다운로드\n            artifact_dir = model_artifact.download(root=download_dir)\n            model_path = f\"{artifact_dir}/{expected_model_path}\"\n            print(f\"\\n✓ Model downloaded from W&B artifacts\")\n            print(f\"  Path: {model_path}\")\n            return model_path\n        else:\n            print(f\"\\n⚠ No model artifact found. Check local file: {expected_model_path}\")\n            return expected_model_path\n\n    except Exception as e:\n        print(f\"\\n⚠ Could not download from artifacts: {e}\")\n        print(f\"  Try using local file: {expected_model_path}\")\n        return expected_model_path\n\n\nprint(\"✓ Best model finder functions ready\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PNIewW1pTwGN",
    "outputId": "23ff25f3-d06b-4389-fbfc-993131ccf40e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def predict_distribution(model, sample_scores, device='cuda'):\n    \"\"\"\n    Predict full distribution from sample scores.\n\n    Args:\n        model: Trained model\n        sample_scores: Array or list of sample scores (0-100 scale)\n        device: Device to run inference on\n\n    Returns:\n        predicted_hist: Predicted histogram distribution (10 bins)\n    \"\"\"\n    model.eval()\n\n    # Preprocess input\n    sample_scores = np.array(sample_scores)\n    sample_scores = np.sort(sample_scores)\n    sample_scores_norm = sample_scores / 100.0\n\n    # Convert to tensor\n    x = torch.tensor(sample_scores_norm, dtype=torch.float32).unsqueeze(0).to(device)\n\n    # Predict\n    with torch.no_grad():\n        predicted_hist = model(x)\n\n    return predicted_hist.cpu().numpy()[0]\n\n\ndef visualize_prediction(sample_scores, predicted_hist):\n    \"\"\"Visualize sample scores and predicted distribution.\"\"\"\n    import matplotlib.pyplot as plt\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n    # Sample scores\n    ax1.scatter(range(len(sample_scores)), sample_scores, alpha=0.6, s=100)\n    ax1.set_xlabel('Student Index')\n    ax1.set_ylabel('Score')\n    ax1.set_title(f'Sample Scores (n={len(sample_scores)})')\n    ax1.grid(alpha=0.3)\n    ax1.set_ylim(0, 100)\n\n    # Predicted distribution\n    bins = np.linspace(0, 100, 11)\n    bin_centers = (bins[:-1] + bins[1:]) / 2\n    ax2.bar(bin_centers, predicted_hist, width=8, alpha=0.7, color='steelblue', edgecolor='black')\n    ax2.set_xlabel('Score Range')\n    ax2.set_ylabel('Probability')\n    ax2.set_title('Predicted Distribution')\n    ax2.grid(alpha=0.3, axis='y')\n    ax2.set_xlim(0, 100)\n\n    plt.tight_layout()\n    plt.show()\n\n\nprint(\"✓ Prediction and visualization functions ready\")",
   "metadata": {
    "id": "RLHJN8zc6EvV",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c9c555ed-8da8-4a71-e901-5a039c234065"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def load_best_model(model_path: str, device='cuda'):\n    \"\"\"\n    Load the best model from checkpoint.\n\n    Args:\n        model_path: Path to the model checkpoint (e.g., 'best_model_xxxxx.pt')\n        device: Device to load the model on\n\n    Returns:\n        model: Loaded model\n        checkpoint: Full checkpoint dictionary with config and metadata\n    \"\"\"\n    # Load checkpoint\n    checkpoint = torch.load(model_path, map_location=device)\n\n    # Extract config\n    config = checkpoint['config']\n\n    # Create model with saved config\n    model = FlexibleHistogramPredictor(\n        hidden_dim=config['hidden_dim'],\n        num_bins=10,\n        num_heads=config['num_heads'],\n        num_inducers=config['num_inducers'],\n        dropout=config['dropout']\n    )\n\n    # Load state dict\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model = model.to(device)\n    model.eval()\n\n    print(f\"✓ Model loaded successfully!\")\n    print(f\"  Validation loss: {checkpoint['val_loss']:.4f}\")\n    print(f\"  Epoch: {checkpoint['epoch']}\")\n    print(f\"  Config: {config}\")\n\n    return model, checkpoint\n\n\nprint(\"✓ Model loading function ready\")",
   "metadata": {
    "id": "lcvPigxq6EvV",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "89356e8e-461d-41ea-c05b-d85ef1d4bbfd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Best 모델 찾기 및 다운로드 (sweep_id는 위 셀에서 생성된 값 사용)\nmodel_path = download_best_model_from_wandb(PROJECT_NAME, sweep_id)\n\n# 모델 로드\nmodel, checkpoint = load_best_model(model_path)\n\n# 예측 테스트\nsample_scores = [75, 82, 68, 91, 77, 85, 73, 80, 88, 79]\npredicted_hist = predict_distribution(model, sample_scores)\nvisualize_prediction(sample_scores, predicted_hist)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 891
    },
    "id": "lSxtfwYBT-Ss",
    "outputId": "c30a8309-ad9d-43ee-81e7-480836d30e85"
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}